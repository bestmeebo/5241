import pandas as pd
import numpy as np
import re  

names_file = "communities.names"
column_names = []
df = pd.read_csv("communities.data", names=column_names, header=None, na_values="?")

df.head()

names_file = "communities.names"

with open(names_file, "r") as f:
    lines = f.readlines()

print(f"ðŸ“Œ `communities.names` total: {len(lines)} ")

print("\n===== `communities.names` content =====")
for i, line in enumerate(lines):
    print(f"{i+1}: {line.strip()}")  
import re

names_file = "communities.names"
column_names = []

with open(names_file, "r") as f:
    for line in f:
        line = line.strip()

       
        if line.startswith("@attribute"):
            column_name = line.split()[1].strip()  

            column_name = re.sub(r"[^a-zA-Z0-9_]", "", column_name)
            column_name = column_name.replace("-", "").replace(" ", "_")

            column_names.append(column_name)



print("top 10 column names:", column_names[:10])
df_temp = pd.read_csv("communities.data", header=None, na_values="?")
print(f"ðŸ“Œ `communities.data` æ–‡ä»¶å®žé™…æœ‰ {df_temp.shape[1]} åˆ—")

df = pd.read_csv("communities.data", names=column_names, header=None, na_values="?")
print(df.select_dtypes(include=['object']).columns)
df['communityname'] = df['communityname'].astype(str)
df.head()
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

print(df["ViolentCrimesPerPop"].describe())
print(df["ViolentCrimesPerPop"].isnull().sum())
print(df["ViolentCrimesPerPop"].unique())

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df["ViolentCrimesPerPop"].dropna(), kde=True, bins=30)
plt.show()

import matplotlib.pyplot as plt

df.isnull().sum().plot(kind="bar", figsize=(12, 6))
plt.title("NaN count per column")
plt.show()
threshold = 1000  # è®¾ç½®é˜ˆå€¼
cols_to_drop = df.isnull().sum()[df.isnull().sum() > threshold].index  # èŽ·å–éœ€è¦åˆ é™¤çš„åˆ—å
df_cleaned = df.drop(columns=cols_to_drop)  # åˆ é™¤è¿™äº›åˆ—

print(f"delect {len(cols_to_drop)} ï¼Œrest of column {df_cleaned.shape[1]} ")
num_rows_with_nan = df.isnull().any(axis=1).sum()
print(f"åŒ…å«ç¼ºå¤±å€¼çš„è¡Œæ•°: {num_rows_with_nan}")
num_empty_rows = df.isnull().all(axis=1).sum()
print(f"å®Œå…¨ç©ºçš„è¡Œæ•°: {num_empty_rows}")
df["missing_count"] = df.isnull().sum(axis=1)
df_sorted = df.sort_values("missing_count", ascending=False)
print(df_sorted[["missing_count"]].head(10))  # æŸ¥çœ‹å‰10è¡Œ
df["crime_category"] = pd.qcut(df["ViolentCrimesPerPop"], q=3, labels=["Low", "Medium", "High"])

# è®¡ç®—æ¯è¡Œï¼ˆç¤¾åŒºï¼‰çš„ç¼ºå¤±çŽ‡
df["missing_ratio"] = df["missing_count"] / df.shape[1]  # é™¤ä»¥æ€»ç‰¹å¾æ•°

# å®šä¹‰ç¼ºå¤±çŽ‡åŒºé—´ï¼ˆ10%, 20%, ..., 90%ï¼‰
bins = np.arange(0, 1.1, 0.1)  # [0%, 10%, ..., 100%]
df["missing_bin"] = pd.cut(df["missing_ratio"], bins, labels=[f"{int(i*100)}%" for i in bins[:-1]])

# æŸ¥çœ‹ä¸åŒç¼ºå¤±çŽ‡åŒºé—´çš„ç¤¾åŒºæ•°é‡
print(df["missing_bin"].value_counts().sort_index())

# æŒ‰åŒºé—´è®¡ç®—ç›®æ ‡å˜é‡å‡å€¼
grouped = df.groupby("missing_bin")["ViolentCrimesPerPop"].mean()
print(grouped)

crime_missing = df.groupby(["crime_category", "missing_bin"]).size().unstack()
print(crime_missing)
crime_missing.plot(kind="bar", stacked=True, figsize=(10,6))

missing_values1 = df_cleaned.isnull().sum()
missing_values1 = missing_values[missing_values > 0]  # åªæ˜¾ç¤ºæœ‰ç¼ºå¤±å€¼çš„åˆ—
print(missing_values1)
df_cleaned.fillna(df_cleaned.select_dtypes(include=['number']).median(), inplace=True)
df_cleaned.fillna(df_cleaned.select_dtypes(include=['object']).mode().iloc[0], inplace=True)
df_cleaned.info()
print(df_cleaned.isnull().sum().sum())  # ç»Ÿè®¡æ•´ä¸ª DataFrame è¿˜æœ‰å¤šå°‘ä¸ª NaN

import statsmodels.api as sm
import pandas as pd

print(df_cleaned.dtypes[df_cleaned.dtypes == 'object'])
X = df_cleaned.drop(columns=["ViolentCrimesPerPop", "communityname"])
y = df_cleaned["ViolentCrimesPerPop"]

X = sm.add_constant(X)  # æ·»åŠ æˆªè·é¡¹
model = sm.OLS(y, X).fit()

# èŽ·å– p-values
p_values = model.pvalues

# é€‰å‡ºå‰ 10 ä¸ªæœ€é‡è¦çš„ç‰¹å¾
top_10_features = p_values.sort_values().iloc[1:11]  # è·³è¿‡ const
print(top_10_features)

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# è®¡ç®— RÂ² å’Œ Adjusted RÂ²
r2 = model.rsquared
adj_r2 = model.rsquared_adj
print(f"RÂ²: {r2:.4f}, Adjusted RÂ²: {adj_r2:.4f}")

# è®¡ç®— VIFï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data)

# æ˜¾è‘—æ€§æ£€éªŒï¼ˆF-statisticï¼‰
print(f"F-statistic: {model.fvalue:.4f}, p-value: {model.f_pvalue:.4f}")

# æ£€æŸ¥æ®‹å·®åˆ†å¸ƒ
import matplotlib.pyplot as plt
import seaborn as sns

sns.histplot(model.resid, kde=True)
plt.title("Residual Distribution")
plt.show()
df_cleaned = df_cleaned.drop(columns=["communityname"])

from itertools import combinations
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# 1. é¢„å¤„ç†æ•°æ®
X = df_cleaned.drop(columns=["ViolentCrimesPerPop", "population", "householdsize"])
y = df_cleaned["ViolentCrimesPerPop"]
!pip install mlxtend

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LinearRegression

# å®šä¹‰çº¿æ€§å›žå½’æ¨¡åž‹
model = LinearRegression()

# å‰å‘é€‰æ‹©ï¼ˆé€æ­¥æ·»åŠ ç‰¹å¾ï¼‰
sfs = SFS(model,
          forward=True,  # True: é€æ­¥å¢žåŠ ç‰¹å¾ï¼ŒFalse: é€æ­¥å‰”é™¤ç‰¹å¾
          floating=False,
          scoring='r2',
          cv=5)  # äº¤å‰éªŒè¯æ¬¡æ•°

# è¿è¡Œç‰¹å¾é€‰æ‹©
sfs.fit(X, y)

# è¾“å‡ºæœ€ä½³ç‰¹å¾
best_features = list(sfs.k_feature_names_)
print(f"Best Subset Features: {best_features}")

pip install --upgrade scikit-learn

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# çº¿æ€§å›žå½’æ¨¡åž‹
model = LinearRegression()

# è®¾å®š RFE é€‰æ‹© 10 ä¸ªæœ€ä½³ç‰¹å¾
selector = RFE(model, n_features_to_select=10)
selector.fit(X, y)

# èŽ·å–æœ€ä½³ç‰¹å¾
selected_features = X.columns[selector.support_]
print(f"RFE Selected Features: {list(selected_features)}")

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression

# Best Subset Selection æ¨¡åž‹
X_best = X[best_features]  # Best Subset é€‰å‡ºçš„ç‰¹å¾
model_best = LinearRegression().fit(X_best, y)
y_pred_best = model_best.predict(X_best)

# RFE æ¨¡åž‹
X_rfe = X[selected_features]  # RFE é€‰å‡ºçš„ç‰¹å¾
model_rfe = LinearRegression().fit(X_rfe, y)
y_pred_rfe = model_rfe.predict(X_rfe)

# è®¡ç®— RÂ² å’Œ MSE
r2_best = r2_score(y, y_pred_best)
mse_best = mean_squared_error(y, y_pred_best)

r2_rfe = r2_score(y, y_pred_rfe)
mse_rfe = mean_squared_error(y, y_pred_rfe)

print(f"Best Subset - RÂ²: {r2_best:.4f}, MSE: {mse_best:.4f}")
print(f"RFE - RÂ²: {r2_rfe:.4f}, MSE: {mse_rfe:.4f}")

from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ alpha
lasso = LassoCV(cv=5, max_iter=5000).fit(X_scaled, y)


# é€‰æ‹©éžé›¶ç³»æ•°å¯¹åº”çš„ç‰¹å¾
lasso_features = X.columns[lasso.coef_ != 0]
print(f"Lasso Selected Features: {list(lasso_features)}")

# è®¡ç®— RÂ² å’Œ MSE
y_pred_lasso = lasso.predict(X_scaled)
r2_lasso = r2_score(y, y_pred_lasso)
mse_lasso = mean_squared_error(y, y_pred_lasso)

print(f"Lasso - RÂ²: {r2_lasso:.4f}, MSE: {mse_lasso:.4f}")

from sklearn.linear_model import ElasticNetCV

# ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ alpha å’Œ l1_ratio
elastic_net = ElasticNetCV(cv=5, l1_ratio=[0.1, 0.5, 0.9]).fit(X_scaled, y)

# é€‰æ‹©éžé›¶ç³»æ•°å¯¹åº”çš„ç‰¹å¾
elastic_net_features = X.columns[elastic_net.coef_ != 0]
print(f"Elastic Net Selected Features: {list(elastic_net_features)}")

# è®¡ç®— RÂ² å’Œ MSE
y_pred_elastic = elastic_net.predict(X_scaled)
r2_elastic = r2_score(y, y_pred_elastic)
mse_elastic = mean_squared_error(y, y_pred_elastic)

print(f"Elastic Net - RÂ²: {r2_elastic:.4f}, MSE: {mse_elastic:.4f}")


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
from sklearn.linear_model import ElasticNet
model_elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000)
model_elastic.fit(X_scaled, y)
# èŽ·å–éžé›¶ç³»æ•°çš„ç‰¹å¾ï¼ˆé€‰ä¸­çš„ç‰¹å¾ï¼‰
elastic_features = X.columns[model_elastic.coef_ != 0]

# è®¡ç®— RÂ² å’Œ MSE
y_pred_elastic = model_elastic.predict(X_scaled)
r2_elastic = r2_score(y, y_pred_elastic)
mse_elastic = mean_squared_error(y, y_pred_elastic)

# è¾“å‡ºç»“æžœ
print(f"Elastic Net Selected Features: {list(elastic_features)}")
print(f"Elastic Net â€“ RÂ²: {r2_elastic:.4f}, MSE: {mse_elastic:.4f}")
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.preprocessing import StandardScaler

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å®šä¹‰ä¸€ç³»åˆ—çš„ alpha å€¼ï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰
alphas = np.logspace(-4, 1, 50)  

# ========== Lasso Path ==========
lasso_coefs = []
for alpha in alphas:
    model = Lasso(alpha=alpha, max_iter=5000)
    model.fit(X_scaled, y)
    lasso_coefs.append(model.coef_)

lasso_coefs = np.array(lasso_coefs)

plt.figure(figsize=(8, 5))
plt.plot(alphas, lasso_coefs)
plt.xscale("log")
plt.xlabel("Alpha")
plt.ylabel("Coefficients")
plt.title("Lasso Regularization Path")
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.preprocessing import StandardScaler

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å®šä¹‰ä¸€ç³»åˆ—çš„ alpha å€¼ï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰
alphas = np.logspace(-4, 1, 50)  

# ========== Elastic Net Path (Two alphas) ==========
elastic_coefs_1 = []
elastic_coefs_2 = []
alpha1, alpha2 = 0.01, 0.1  # é€‰å–ä¸¤ä¸ªä¸åŒçš„ alpha

for alpha in alphas:
    model1 = ElasticNet(alpha=alpha1, l1_ratio=0.5, max_iter=5000)
    model2 = ElasticNet(alpha=alpha2, l1_ratio=0.5, max_iter=5000)
    
    model1.fit(X_scaled, y)
    model2.fit(X_scaled, y)
    
    elastic_coefs_1.append(model1.coef_)
    elastic_coefs_2.append(model2.coef_)

elastic_coefs_1 = np.array(elastic_coefs_1)
elastic_coefs_2 = np.array(elastic_coefs_2)

plt.figure(figsize=(10, 6))  # å¢žå¤§å›¾çš„å°ºå¯¸
for i in range(min(10, X.shape[1])):  # åªç»˜åˆ¶å‰10ä¸ªç‰¹å¾ï¼Œé¿å…å¤ªå¤šå›¾ä¾‹
    plt.plot(alphas, elastic_coefs_1[:, i], label=f'Alpha={alpha1}, Feature {i}')
    plt.plot(alphas, elastic_coefs_2[:, i], linestyle='dashed', label=f'Alpha={alpha2}, Feature {i}')

plt.xscale("log")
plt.xlabel("Alpha")
plt.ylabel("Coefficients")
plt.title("Elastic Net Regularization Path")

# **è°ƒæ•´å›¾ä¾‹**
plt.legend(loc="upper left", bbox_to_anchor=(1, 1), fontsize=9, frameon=True)  # å›¾ä¾‹æ”¾åˆ°å›¾å¤–
plt.grid(True)

# **è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢å›¾ä¾‹è¶…å‡º**
plt.tight_layout(rect=[0, 0, 0.75, 1])  # è®©å›¾ä¾‹åœ¨å³ä¾§ä¸è¶…å‡ºè¾¹ç•Œ

plt.show()

# ========== Ridge Path ==========
ridge_coefs = []
for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_scaled, y)
    ridge_coefs.append(model.coef_)

ridge_coefs = np.array(ridge_coefs)

plt.figure(figsize=(8, 5))
plt.plot(alphas, ridge_coefs)
plt.xscale("log")
plt.xlabel("Alpha")
plt.ylabel("Coefficients")
plt.title("Ridge Regularization Path")
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import RFE
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

# åŠ è½½æ•°æ®
X = df_cleaned.drop(columns=["ViolentCrimesPerPop"])
y = df_cleaned["ViolentCrimesPerPop"]

# è®¾ç½®é‡å¤å®žéªŒæ¬¡æ•°
n_iterations = 10
results = {"Least Squares": [], "Ridge": [], "Best Subset": [], "Stepwise RFE": [], "Lasso": [], "Elastic Net": []}

# è¿è¡Œ10æ¬¡å®žéªŒ
for _ in range(n_iterations):
    # æ‹†åˆ†æ•°æ®é›† (60% train, 20% validation, 20% test)
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=None)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=None)

    #### 1. æœ€å°äºŒä¹˜æ³• OLS ####
    model_ols = LinearRegression().fit(X_train, y_train)
    y_pred_ols = model_ols.predict(X_test)
    results["Least Squares"].append(mean_squared_error(y_test, y_pred_ols))

    #### 2. å²­å›žå½’ Ridge (è¶…å‚æ•°è°ƒä¼˜) ####
    best_alpha = min([0.01, 0.1, 1, 10, 100], key=lambda a: mean_squared_error(y_val, Ridge(alpha=a).fit(X_train, y_train).predict(X_val)))
    model_ridge = Ridge(alpha=best_alpha).fit(X_train, y_train)
    results["Ridge"].append(mean_squared_error(y_test, model_ridge.predict(X_test)))

    #### 3. æœ€ä¼˜å­é›†é€‰æ‹© Best Subsets ####
    sfs = SFS(LinearRegression(), k_features=10, forward=True, floating=False, scoring='neg_mean_squared_error', cv=5)
    sfs.fit(X_train, y_train)
    selected_features = list(sfs.k_feature_names_)
    model_best_subset = LinearRegression().fit(X_train[selected_features], y_train)
    results["Best Subset"].append(mean_squared_error(y_test, model_best_subset.predict(X_test[selected_features])))

    #### 4. é€æ­¥å›žå½’ (RFE) ####
    selector = RFE(LinearRegression(), n_features_to_select=10)
    selector.fit(X_train, y_train)
    X_train_rfe, X_test_rfe = X_train.iloc[:, selector.support_], X_test.iloc[:, selector.support_]
    model_rfe = LinearRegression().fit(X_train_rfe, y_train)
    results["Stepwise RFE"].append(mean_squared_error(y_test, model_rfe.predict(X_test_rfe)))

    #### 5. Lasso å›žå½’ (è¶…å‚æ•°è°ƒä¼˜) ####
    best_alpha = min([0.01, 0.1, 1, 10], key=lambda a: mean_squared_error(y_val, Lasso(alpha=a).fit(X_train, y_train).predict(X_val)))
    model_lasso = Lasso(alpha=best_alpha).fit(X_train, y_train)
    results["Lasso"].append(mean_squared_error(y_test, model_lasso.predict(X_test)))

    #### 6. Elastic Net (è¶…å‚æ•°è°ƒä¼˜) ####
    best_alpha = min([0.01, 0.1, 1, 10], key=lambda a: mean_squared_error(y_val, ElasticNet(alpha=a, l1_ratio=0.5).fit(X_train, y_train).predict(X_val)))
    model_elastic = ElasticNet(alpha=best_alpha, l1_ratio=0.5).fit(X_train, y_train)
    results["Elastic Net"].append(mean_squared_error(y_test, model_elastic.predict(X_test)))

# è®¡ç®—10æ¬¡å®žéªŒçš„å¹³å‡MSE
avg_results = {method: np.mean(mse_list) for method, mse_list in results.items()}

# æ˜¾ç¤ºç»“æžœ
df_results = pd.DataFrame.from_dict(avg_results, orient="index", columns=["Mean Test MSE"])
print(df_results)

# ç»˜åˆ¶æŸ±çŠ¶å›¾
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.barh(df_results.index, df_results["Mean Test MSE"], color=['blue', 'red', 'green', 'purple', 'orange', 'cyan'])
plt.xlabel("Mean Test MSE (Lower is better)")
plt.title("Comparison of Linear Methods for Prediction")
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate simulated data
np.random.seed(42)
X = np.random.rand(100, 2)  # Two features
y = 3 + 5 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100) * 0.5  # Linear relationship with noise

# Method 1: Standard OLS regression
X_ols = sm.add_constant(X)  # Add intercept term
model_ols = sm.OLS(y, X_ols).fit()
print("Standard OLS coefficients:", model_ols.params)

# Method 2: Centering X and y before regression
X_centered = X - np.mean(X, axis=0)
y_centered = y - np.mean(y)
model_centered = sm.OLS(y_centered, X_centered).fit()
print("Centered OLS coefficients (excluding intercept):", model_centered.params)

# Method 3: Adding a column of ones to X
X_augmented = np.column_stack((np.ones(X.shape[0]), X))
model_augmented = sm.OLS(y, X_augmented).fit()
print("OLS with added constant column:", model_augmented.params)

# Generate data where p > n
n, p = 5, 10  # More features than samples
X_high_dim = np.random.rand(n, p)
y_high_dim = np.random.rand(n)

# Train OLS regression
model_high_dim = LinearRegression().fit(X_high_dim, y_high_dim)
y_pred = model_high_dim.predict(X_high_dim)

# Compute training error
train_error = np.mean((y_high_dim - y_pred) ** 2)
print("Training error (p > n):", train_error)

import seaborn as sns
import matplotlib.pyplot as plt

# Generate highly correlated features
X_corr = np.random.rand(100, 1)
X_corr = np.hstack([X_corr, X_corr + np.random.normal(0, 0.01, (100, 1))])  # Highly correlated
y_corr = 3 + 5 * X_corr[:, 0] + 2 * X_corr[:, 1] + np.random.randn(100) * 0.5

# OLS regression
model_corr = sm.OLS(y_corr, sm.add_constant(X_corr)).fit()
print("OLS coefficients for correlated variables:", model_corr.params)

# Visualize feature correlation
sns.heatmap(np.corrcoef(X_corr.T), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Matrix")
plt.show()

from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=10).fit(X_corr, y_corr)
print("Ridge regression coefficients:", ridge_model.coef_)

from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.1).fit(X_corr, y_corr)
print("Lasso regression coefficients:", lasso_model.coef_)


